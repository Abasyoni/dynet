**Model**: CPU VS GPU

**Input**: x = 1000 dimensional real vector

**Output**: y = 1000 dimensional real vector

**Objective Function**: *a linear transformation W such that |W^10-W| is minimal*

**Training Data**: *(W, W^10) where W is initially arbitrary but is continuously updated as the parameters are tuned*

**Model**: *Reinforcement Learning*


**Nodes**: 
1. There is only one node which is a 10 x 10 matrix.

**Description**: This model is mainly used to illustrate the perforance difference between training on the CPU and training on the GPU.

